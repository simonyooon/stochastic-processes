This project discusses how to find and estimate the minimum mean square error
of random variables. Part 1 analyzes how to find both the linear MMSE and
Bayesian MMSE estimators when analyzing one observation from a summation of two
uniformly distributed random variables. Part 2 finds the linear estimator
for a similar situation except with an arbitrary number of observations.
Clearly, as the number of noisy observations for a given Y value increase, so
does the accuracy of the guess. This subsequently causes a decrease in
MSE value of the MMSE estimator.  The variance also plays a role in the MSE
as a low variance provides smaller discrepancies in estimates and thus
a lower MSE. This information is corroborated in the generated graph in Part 2.
