
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>stoch4</title><meta name="generator" content="MATLAB 9.10"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2022-04-20"><meta name="DC.source" content="stoch4.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">Question 1: Detection Algorithm</a></li><li><a href="#3">Question 2:</a></li><li><a href="#4">Functions for Question 1</a></li></ul></div><pre class="codeinput"><span class="comment">% Michael Bentivegna, Simon Yoon, Joya Debi</span>
<span class="comment">% ECE302 Stochastic Processes Project 4: Detection</span>

clear;
clc;
close <span class="string">all</span>;

<span class="comment">% Summary of Project</span>

<span class="comment">% Question 1 looks at how to model a radar detector using MAP decision</span>
<span class="comment">% rules.  Part A finds the optimal decision boundary for detection and</span>
<span class="comment">% finds the theoretical probability of error given the decision rule.</span>
<span class="comment">% These values were then compared to the theoretical values that were found</span>
<span class="comment">% mathematically.  Part B then plots the receiver operating curve for</span>
<span class="comment">% differing signal to noise ratios.  As expected, the higher the signal to</span>
<span class="comment">% noise ratio, the better the detection algorithm performed.  Part C</span>
<span class="comment">% specifies missing a target is 10 times worse than a false positive</span>
<span class="comment">% judgement.  This updated our part A answers and the differing decision</span>
<span class="comment">% boundaries were displayed on a new ROC graph.  The new decision boundary</span>
<span class="comment">% had a higher tolerance for false positive in comparison to Part A, which</span>
<span class="comment">% logically agrees with the updated cost. Part E causes the target to no longer</span>
<span class="comment">% have a unique mean, but rather a unique variance.  This caused there to</span>
<span class="comment">% be two decision boundaries, with the target classified in the middle region.</span>
<span class="comment">% A new decision boundary, error, and ROC function were created to settle</span>
<span class="comment">% this case.  The experimental values again agreed with the theoretical</span>
<span class="comment">% results.  Question 2 introduces a new dataset that utilized</span>
<span class="comment">% likelihood and prior information for classification.  The data</span>
<span class="comment">% was first split up into a testing and training dataset. The training</span>
<span class="comment">% dataset used it's known classes to make a 4D pdf for each case.  The</span>
<span class="comment">% testing data was then input into each pdf and multiplied by the corresponding</span>
<span class="comment">% bayesian prior to get it's probability of being a member of that class.</span>
<span class="comment">% The class with the highest probability was selected and the corresponding</span>
<span class="comment">% confusion matrix was plotted.</span>
</pre><h2 id="2">Question 1: Detection Algorithm</h2><pre class="codeinput"><span class="comment">% ***********Part A*************</span>

<span class="comment">% Theoretical Values</span>
fprintf(<span class="string">'Theoretical Decision Boundary for Part A = 1.693. For any samples greater than this value the target will be predicted to be present.\n'</span>)
fprintf(<span class="string">'Theoretical Probability of Error for Part A = .112 \n\n'</span>)

<span class="comment">% The theoretical decision boundary was found by plotting the two gaussian functions</span>
<span class="comment">% with their prior likelihood and seeing where they intersected.</span>

<span class="comment">% The theoretical probability of error was found by finding the area under each curve</span>
<span class="comment">% where either a false positive or false negative could occur. This was</span>
<span class="comment">% done through definite integral calculation.</span>

<span class="comment">% Known Values</span>
A = 2;
Z = 0;
var = 1;
probWithout = .8;

<span class="comment">% Call Decision Region Function</span>
decisionRegion = findDecisionRegion(Z, var, A, var, probWithout, 1);
fprintf(<span class="string">'Experimental Decision Region for Part A = %f. For any samples greater than this value the target will be predicted to be present.\n'</span>, decisionRegion)

<span class="comment">% Call Experimental Probability of Error Function</span>
error = experimentalProbError(Z, var, A, var, probWithout, decisionRegion);
fprintf(<span class="string">'Experimental Probability of Error for Part A = %f \n'</span>, error)

<span class="comment">% ***********Part B*************</span>

<span class="comment">%Call function for different ROC values (note the zero in the last parameter this will be used in part c)</span>
[falsePos1, truePos1] = ROCcurve(Z, .5*var, A, .5*var, probWithout, 0);
[falsePos2, truePos2] = ROCcurve(Z, var, A, var, probWithout, 0);
[falsePos3, truePos3] = ROCcurve(Z, 2*var, A, 2*var, probWithout, 0);

<span class="comment">% Plotting ROC for different SNR</span>
figure(1)
hold <span class="string">on</span>;
plot(falsePos1, truePos1)
plot(falsePos2, truePos2)
plot(falsePos3, truePos3)
legend(<span class="string">'SNR = 4'</span>, <span class="string">'SNR = 2'</span>, <span class="string">'SNR = 1'</span>)
xlabel(<span class="string">"False Positive Rate"</span>)
ylabel(<span class="string">"True Positive Rate"</span>)
title(<span class="string">"ROC Curve for Different SNRs"</span>)

<span class="comment">% ***********Part C*************</span>

<span class="comment">% Now input a cost other than 1 in the decision region function</span>
decisionRegionC = findDecisionRegion(Z, var, A, var, probWithout, 10);
[falsePosC, truePosC, specificSampleCoord] = ROCcurve(Z, var, A, var, probWithout, [decisionRegionC, decisionRegion]);


<span class="comment">% Plotting ROC w/ optimal Coordinate Labelled</span>
figure(2)
hold <span class="string">on</span>;
plot(falsePosC, truePosC)
plot(specificSampleCoord(1,1), specificSampleCoord(1,2), <span class="string">'o'</span>)
plot(specificSampleCoord(2,1), specificSampleCoord(2,2), <span class="string">'o'</span>)
xlabel(<span class="string">"False Positive Rate"</span>)
ylabel(<span class="string">"True Positive Rate"</span>)
title(<span class="string">"ROC Curve"</span>)
legend(<span class="string">"ROC Curve w/ SNR = 2"</span>, <span class="string">"Optimal Marker w/ C_{01} = 10*C_{10}"</span>, <span class="string">"Optimal Marker w/ C_{01} = C_{10}"</span>)

<span class="comment">% ***********Part D*************</span>

<span class="comment">% Omitted</span>

<span class="comment">% ***********Part E*************</span>

<span class="comment">%Theoretically Calculated Values (found the same was as in part A)</span>
fprintf(<span class="string">'\n***************************\n\n'</span>)
fprintf(<span class="string">'Theoretical Decision Region for Part E = -1.187 to 1.187.  For any samples in this region the target will be predicted to be present. \n'</span>)
fprintf(<span class="string">'Theoretical Probability of Error for Part E = .1414 \n\n'</span>)
<span class="comment">% Known Values</span>
Ae = 0;
varWithout = 8;
varTarget = 1;
probWithoute = .8;

<span class="comment">% Call decision region function</span>
decisionRegionE = findDecisionRegionE(Ae, varWithout, varTarget, probWithoute);
fprintf(<span class="string">'Experimental Decision Region for Part E = %f to %f. For any samples in this region the target will be predicted to be present.\n'</span>, decisionRegionE(1), decisionRegionE(2))

<span class="comment">% Call error function</span>
errorE = experimentalProbErrorE(Ae, varWithout, varTarget, probWithoute, decisionRegionE);
fprintf(<span class="string">'Experimental Probability of Error for Part E = %f \n'</span>, errorE)

<span class="comment">%Call ROC curve functions</span>
[falsePosE1, truePosE1] = ROCcurveE(Ae, 16, 1, probWithoute);
[falsePosE2, truePosE2] = ROCcurveE(Ae, 8, 1, probWithoute);
[falsePosE3, truePosE3] = ROCcurveE(Ae, 4, 1, probWithoute);

<span class="comment">%Plotting</span>
figure(3)
hold <span class="string">on</span>;
plot(falsePosE1, truePosE1)
plot(falsePosE2, truePosE2)
plot(falsePosE3, truePosE3)
legend(<span class="string">'Var Ratio = 16'</span>, <span class="string">'Var Ratio = 8'</span>, <span class="string">'Var Ratio = 4'</span>)
xlabel(<span class="string">"False Positive Rate"</span>)
ylabel(<span class="string">"True Positive Rate"</span>)
title(<span class="string">"ROC Curve for Different Variances"</span>)
hold <span class="string">off</span>;
</pre><pre class="codeoutput">Theoretical Decision Boundary for Part A = 1.693. For any samples greater than this value the target will be predicted to be present.
Theoretical Probability of Error for Part A = .112 

</pre><img vspace="5" hspace="5" src="stoch4_01.png" alt=""> <img vspace="5" hspace="5" src="stoch4_02.png" alt=""> <img vspace="5" hspace="5" src="stoch4_03.png" alt=""> <h2 id="3">Question 2:</h2><pre class="codeinput"><span class="comment">% Load in Iris data (comes with features and labels variables)</span>
load(<span class="string">'Iris.mat'</span>);

<span class="comment">% 150 4D samples</span>
dimOfData = size(features);
samples = dimOfData(1);

<span class="comment">% 3 Unique labels (1, 2, 3)</span>
uniqueLabels = length(unique(labels));

<span class="comment">% Split data into training and testing</span>
halfSamples = samples/2;
shuffling = transpose(randperm(samples));

training = shuffling(1:halfSamples);
testing = shuffling(halfSamples+1:samples);

<span class="comment">%Get training and testing data</span>
trainFeatures = features(training, :);
trainLabels = labels(training, :);

testFeatures = features(testing, :);
testLabels = labels(testing, :);

<span class="comment">% Classify each 4D sample with its correct label in the training data and get its mean and covariance</span>
featuresForOne = trainFeatures((trainLabels == 1),:);
featuresForTwo = trainFeatures((trainLabels == 2),:);
featuresForThree = trainFeatures((trainLabels == 3),:);

meanOne = mean(featuresForOne);
meanTwo = mean(featuresForTwo);
meanThree = mean(featuresForThree);

covOne = cov(featuresForOne);
covTwo = cov(featuresForTwo);
covThree = cov(featuresForThree);

<span class="comment">% Get prior values for each label (since the labels are 1,2,3 they are also in the same spot as their index</span>
bayesianPriors = histcounts(testLabels) / length(testLabels);

<span class="comment">% Probability of test features to come from each of the three labels using training data's mean and covariance</span>
probOfEach = zeros([halfSamples, uniqueLabels]);
probOfEach(:,1) = mvnpdf(testFeatures, meanOne, covOne) * bayesianPriors(1);
probOfEach(:,2) = mvnpdf(testFeatures, meanTwo, covTwo) * bayesianPriors(2);
probOfEach(:,3) = mvnpdf(testFeatures, meanThree, covThree) * bayesianPriors(3);

<span class="comment">% Choose the best label for each samples and see the error rate</span>
[maximum, columnIndex] = max(probOfEach, [], 2);
errorProb = 1 - mean(columnIndex == testLabels);
fprintf(<span class="string">"\nThe Probability of Classification Error: %f"</span>, errorProb);

<span class="comment">%Plot Confusion Matrix</span>
figure(4);
confusionchart(confusionmat(columnIndex, testLabels));
title(<span class="string">'Confusion Matrix for Iris Data'</span>);
</pre><pre class="codeoutput">
The Probability of Classification Error: 0.013333</pre><img vspace="5" hspace="5" src="stoch4_04.png" alt=""> <h2 id="4">Functions for Question 1</h2><pre class="codeinput"><span class="comment">% Find border of decision regions for Part 1A and 1C</span>
<span class="keyword">function</span> decisionRegion = findDecisionRegion(mu0, var0, mu1, var1, probWithout, costMiss)
    <span class="comment">% Setup</span>
    spacing = 10000;
    x = linspace(-10, 10, spacing);
    probWith = 1 - probWithout;

    <span class="comment">% Get pdf when target is present and when it is not present</span>
    yWithoutDetection = normpdf(x, mu0, var0)*probWithout;
    yWithDetection = normpdf(x, mu1, var1)*probWith*costMiss;

    <span class="comment">% Find where the threshold value is by seeing where the functions intersect</span>
    <span class="keyword">for</span> i = 1:spacing
        <span class="keyword">if</span> (yWithoutDetection(i) &lt;= yWithDetection(i))
            decisionRegion = x(i);
            <span class="keyword">break</span>;
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">%Find experimental probability of error for the decision region in Part 1A</span>
<span class="keyword">function</span> error = experimentalProbError(mu0, var0, mu1, var1, probWithout, decisionRegion)
    <span class="comment">% Setup</span>
    totalSamples = 100000;

    <span class="comment">% Generate proper amount of samples from each distribution</span>
    nonTargetSamples = normrnd(mu0, var0, [1, round(probWithout*totalSamples)]);
    targetSamples = normrnd(mu1, var1, [1, round((1-probWithout)*totalSamples)]);


    <span class="comment">% Check to see how often samples are correctly categorized</span>
    error = 1 - ((length(targetSamples(targetSamples &gt; decisionRegion)) + length(nonTargetSamples(nonTargetSamples &lt; decisionRegion))) / totalSamples);
<span class="keyword">end</span>

<span class="comment">% Find ROC arrays for Part 1A conditions</span>
<span class="keyword">function</span> [falsePositiveRate, truePositiveRate, specificSampleCoord] = ROCcurve(mu0, var0, mu1, var1, probWithout, decisionRegion)
    <span class="comment">% Setup</span>
    totalSamples = 10000;
    spacing = 1000;
    x = linspace(-10, 10, spacing);
    falsePositiveRate = zeros([1, 100]);
    truePositiveRate = zeros([1, 100]);
    specificSampleCoord = zeros([length(decisionRegion), 2]);

    <span class="comment">% Generate proper amount of samples from each distribution</span>
    nonTargetSamples = normrnd(mu0, var0, [1, round(probWithout*totalSamples)]);
    targetSamples = normrnd(mu1, var1, [1, round((1-probWithout)*totalSamples)]);

    <span class="comment">%Run through all possible decision region choices to get ROC curve</span>
    <span class="keyword">for</span> i = 1:spacing
            falsePositiveRate(i) = length(nonTargetSamples(nonTargetSamples &gt; x(i)))/ (round(probWithout*totalSamples));
            truePositiveRate(i) = length(targetSamples(targetSamples &gt; x(i))) / (round((1-probWithout)*totalSamples));
    <span class="keyword">end</span>

    <span class="comment">%Use specific coordinate for the decision regions (mostly for part c)</span>
    <span class="keyword">for</span> i = 1:length(decisionRegion)
        specificSampleCoord(i, 1) = length(nonTargetSamples(nonTargetSamples &gt; decisionRegion(i)))/ (round(probWithout*totalSamples));
        specificSampleCoord(i, 2) = length(targetSamples(targetSamples &gt; decisionRegion(i))) / (round((1-probWithout)*totalSamples));
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">% Find border of decision regions for Part 1E</span>
<span class="keyword">function</span> decisionRegion = findDecisionRegionE(mu0, var0, var1, probWithout)
    <span class="comment">% Setup</span>
    spacing = 10000;
    x = linspace(-10, 10, spacing);
    probWith = 1 - probWithout;

    <span class="comment">% Get pdf when target is present and when it is not present</span>
    yWithoutDetection = normpdf(x, mu0, var0)*probWithout;
    yWithDetection = normpdf(x, mu0, var1)*probWith;

    <span class="comment">% Find where the threshold value is by seeing where the functions intersect</span>
    decisionRegion = zeros([1,2]);
    <span class="keyword">for</span> i = 1:spacing
        <span class="keyword">if</span> (yWithoutDetection(i) &lt;= yWithDetection(i))
            decisionRegion(2) = x(i);
        <span class="keyword">end</span>

        <span class="keyword">if</span> (yWithoutDetection(spacing + 1 - i) &lt;= yWithDetection(spacing + 1 - i))
            decisionRegion(1) = x(spacing + 1 - i);
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">%Find experimental probability of error for the decision region in Part 1E</span>
<span class="keyword">function</span> error = experimentalProbErrorE(mu0, var0, var1, probWithout, decisionRegion)
    <span class="comment">% Setup</span>
    totalSamples = 100000;

    <span class="comment">% Generate proper amount of samples from each distribution</span>
    nonTargetSamples = normrnd(mu0, var0, [1, round(probWithout*totalSamples)]);
    targetSamples = normrnd(mu0, var1, [1, round((1-probWithout)*totalSamples)]);


    <span class="comment">% Check to see how often samples are correctly categorized</span>
    error = (length(nonTargetSamples((nonTargetSamples &lt; decisionRegion(2)) &amp; (nonTargetSamples &gt; decisionRegion(1)))) + length((targetSamples(targetSamples &gt; decisionRegion(2)))) + length((targetSamples(targetSamples &lt; decisionRegion(1))))) / totalSamples;

<span class="keyword">end</span>

<span class="comment">% Find ROC arrays for Part 1E conditions</span>
<span class="keyword">function</span> [falsePositiveRate, truePositiveRate] = ROCcurveE(mu0, var0, var1, probWithout)
    <span class="comment">% Setup</span>
    totalSamples = 10000;
    spacing = 1000;
    x = linspace(-10, 10, spacing);
    falsePositiveRate = zeros([1, 100]);
    truePositiveRate = zeros([1, 100]);

    <span class="comment">% Generate proper amount of samples from each distribution</span>
    nonTargetSamples = normrnd(mu0, var0, [1, round(probWithout*totalSamples)]);
    targetSamples = normrnd(mu0, var1, [1, round((1-probWithout)*totalSamples)]);

    <span class="comment">%Run through all possible decision region choices to get ROC curve</span>
    <span class="keyword">for</span> i = 1:spacing
            falsePositiveRate(i) = length(nonTargetSamples(abs(nonTargetSamples) &lt; abs(x(i))))/ (round(probWithout*totalSamples));
            truePositiveRate(i) = length(targetSamples(abs(targetSamples) &lt; abs(x(i)))) / (round((1-probWithout)*totalSamples));
    <span class="keyword">end</span>

<span class="keyword">end</span>
</pre><pre class="codeoutput">Experimental Decision Region for Part A = 1.693169. For any samples greater than this value the target will be predicted to be present.
Experimental Probability of Error for Part A = 0.111490 

***************************

Theoretical Decision Region for Part E = -1.187 to 1.187.  For any samples in this region the target will be predicted to be present. 
Theoretical Probability of Error for Part E = .1414 

Experimental Decision Region for Part E = -1.185119 to 1.185119. For any samples in this region the target will be predicted to be present.
Experimental Probability of Error for Part E = 0.139210 
</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2021a</a><br></p></div><!--
##### SOURCE BEGIN #####
% Michael Bentivegna, Simon Yoon, Joya Debi
% ECE302 Stochastic Processes Project 4: Detection

clear;
clc;
close all;

% Summary of Project

% Question 1 looks at how to model a radar detector using MAP decision
% rules.  Part A finds the optimal decision boundary for detection and
% finds the theoretical probability of error given the decision rule.
% These values were then compared to the theoretical values that were found
% mathematically.  Part B then plots the receiver operating curve for
% differing signal to noise ratios.  As expected, the higher the signal to
% noise ratio, the better the detection algorithm performed.  Part C
% specifies missing a target is 10 times worse than a false positive
% judgement.  This updated our part A answers and the differing decision
% boundaries were displayed on a new ROC graph.  The new decision boundary
% had a higher tolerance for false positive in comparison to Part A, which
% logically agrees with the updated cost. Part E causes the target to no longer
% have a unique mean, but rather a unique variance.  This caused there to
% be two decision boundaries, with the target classified in the middle region.
% A new decision boundary, error, and ROC function were created to settle
% this case.  The experimental values again agreed with the theoretical
% results.  Question 2 introduces a new dataset that utilized
% likelihood and prior information for classification.  The data
% was first split up into a testing and training dataset. The training
% dataset used it's known classes to make a 4D pdf for each case.  The
% testing data was then input into each pdf and multiplied by the corresponding
% bayesian prior to get it's probability of being a member of that class.
% The class with the highest probability was selected and the corresponding
% confusion matrix was plotted.

%% Question 1: Detection Algorithm

% ***********Part A*************

% Theoretical Values
fprintf('Theoretical Decision Boundary for Part A = 1.693. For any samples greater than this value the target will be predicted to be present.\n')
fprintf('Theoretical Probability of Error for Part A = .112 \n\n')

% The theoretical decision boundary was found by plotting the two gaussian functions
% with their prior likelihood and seeing where they intersected.

% The theoretical probability of error was found by finding the area under each curve
% where either a false positive or false negative could occur. This was
% done through definite integral calculation.

% Known Values
A = 2;
Z = 0;
var = 1;
probWithout = .8;

% Call Decision Region Function
decisionRegion = findDecisionRegion(Z, var, A, var, probWithout, 1);
fprintf('Experimental Decision Region for Part A = %f. For any samples greater than this value the target will be predicted to be present.\n', decisionRegion)

% Call Experimental Probability of Error Function
error = experimentalProbError(Z, var, A, var, probWithout, decisionRegion);
fprintf('Experimental Probability of Error for Part A = %f \n', error)

% ***********Part B*************

%Call function for different ROC values (note the zero in the last parameter this will be used in part c)
[falsePos1, truePos1] = ROCcurve(Z, .5*var, A, .5*var, probWithout, 0);
[falsePos2, truePos2] = ROCcurve(Z, var, A, var, probWithout, 0);
[falsePos3, truePos3] = ROCcurve(Z, 2*var, A, 2*var, probWithout, 0);

% Plotting ROC for different SNR
figure(1)
hold on;
plot(falsePos1, truePos1)
plot(falsePos2, truePos2)
plot(falsePos3, truePos3)
legend('SNR = 4', 'SNR = 2', 'SNR = 1')
xlabel("False Positive Rate")
ylabel("True Positive Rate")
title("ROC Curve for Different SNRs")

% ***********Part C*************

% Now input a cost other than 1 in the decision region function
decisionRegionC = findDecisionRegion(Z, var, A, var, probWithout, 10);
[falsePosC, truePosC, specificSampleCoord] = ROCcurve(Z, var, A, var, probWithout, [decisionRegionC, decisionRegion]);


% Plotting ROC w/ optimal Coordinate Labelled
figure(2)
hold on;
plot(falsePosC, truePosC)
plot(specificSampleCoord(1,1), specificSampleCoord(1,2), 'o')
plot(specificSampleCoord(2,1), specificSampleCoord(2,2), 'o')
xlabel("False Positive Rate")
ylabel("True Positive Rate")
title("ROC Curve")
legend("ROC Curve w/ SNR = 2", "Optimal Marker w/ C_{01} = 10*C_{10}", "Optimal Marker w/ C_{01} = C_{10}")

% ***********Part D*************

% Omitted

% ***********Part E*************

%Theoretically Calculated Values (found the same was as in part A)
fprintf('\n***************************\n\n')
fprintf('Theoretical Decision Region for Part E = -1.187 to 1.187.  For any samples in this region the target will be predicted to be present. \n')
fprintf('Theoretical Probability of Error for Part E = .1414 \n\n')
% Known Values
Ae = 0;
varWithout = 8;
varTarget = 1;
probWithoute = .8;

% Call decision region function
decisionRegionE = findDecisionRegionE(Ae, varWithout, varTarget, probWithoute);
fprintf('Experimental Decision Region for Part E = %f to %f. For any samples in this region the target will be predicted to be present.\n', decisionRegionE(1), decisionRegionE(2))

% Call error function
errorE = experimentalProbErrorE(Ae, varWithout, varTarget, probWithoute, decisionRegionE);
fprintf('Experimental Probability of Error for Part E = %f \n', errorE)

%Call ROC curve functions
[falsePosE1, truePosE1] = ROCcurveE(Ae, 16, 1, probWithoute);
[falsePosE2, truePosE2] = ROCcurveE(Ae, 8, 1, probWithoute);
[falsePosE3, truePosE3] = ROCcurveE(Ae, 4, 1, probWithoute);

%Plotting
figure(3)
hold on;
plot(falsePosE1, truePosE1)
plot(falsePosE2, truePosE2)
plot(falsePosE3, truePosE3)
legend('Var Ratio = 16', 'Var Ratio = 8', 'Var Ratio = 4')
xlabel("False Positive Rate")
ylabel("True Positive Rate")
title("ROC Curve for Different Variances")
hold off;


%% Question 2:

% Load in Iris data (comes with features and labels variables)
load('Iris.mat');

% 150 4D samples
dimOfData = size(features);
samples = dimOfData(1);

% 3 Unique labels (1, 2, 3)
uniqueLabels = length(unique(labels));

% Split data into training and testing
halfSamples = samples/2;
shuffling = transpose(randperm(samples));

training = shuffling(1:halfSamples);
testing = shuffling(halfSamples+1:samples);

%Get training and testing data
trainFeatures = features(training, :);
trainLabels = labels(training, :);

testFeatures = features(testing, :);
testLabels = labels(testing, :);

% Classify each 4D sample with its correct label in the training data and get its mean and covariance
featuresForOne = trainFeatures((trainLabels == 1),:);
featuresForTwo = trainFeatures((trainLabels == 2),:);
featuresForThree = trainFeatures((trainLabels == 3),:);

meanOne = mean(featuresForOne);
meanTwo = mean(featuresForTwo);
meanThree = mean(featuresForThree);

covOne = cov(featuresForOne);
covTwo = cov(featuresForTwo);
covThree = cov(featuresForThree);

% Get prior values for each label (since the labels are 1,2,3 they are also in the same spot as their index
bayesianPriors = histcounts(testLabels) / length(testLabels);

% Probability of test features to come from each of the three labels using training data's mean and covariance
probOfEach = zeros([halfSamples, uniqueLabels]);
probOfEach(:,1) = mvnpdf(testFeatures, meanOne, covOne) * bayesianPriors(1);
probOfEach(:,2) = mvnpdf(testFeatures, meanTwo, covTwo) * bayesianPriors(2);
probOfEach(:,3) = mvnpdf(testFeatures, meanThree, covThree) * bayesianPriors(3);

% Choose the best label for each samples and see the error rate
[maximum, columnIndex] = max(probOfEach, [], 2);
errorProb = 1 - mean(columnIndex == testLabels);
fprintf("\nThe Probability of Classification Error: %f", errorProb);

%Plot Confusion Matrix
figure(4);
confusionchart(confusionmat(columnIndex, testLabels));
title('Confusion Matrix for Iris Data');

%% Functions for Question 1

% Find border of decision regions for Part 1A and 1C
function decisionRegion = findDecisionRegion(mu0, var0, mu1, var1, probWithout, costMiss)
    % Setup
    spacing = 10000;
    x = linspace(-10, 10, spacing);
    probWith = 1 - probWithout;
    
    % Get pdf when target is present and when it is not present
    yWithoutDetection = normpdf(x, mu0, var0)*probWithout;
    yWithDetection = normpdf(x, mu1, var1)*probWith*costMiss;
    
    % Find where the threshold value is by seeing where the functions intersect
    for i = 1:spacing
        if (yWithoutDetection(i) <= yWithDetection(i))
            decisionRegion = x(i);
            break;
        end
    end
end

%Find experimental probability of error for the decision region in Part 1A
function error = experimentalProbError(mu0, var0, mu1, var1, probWithout, decisionRegion)
    % Setup
    totalSamples = 100000;
    
    % Generate proper amount of samples from each distribution
    nonTargetSamples = normrnd(mu0, var0, [1, round(probWithout*totalSamples)]);
    targetSamples = normrnd(mu1, var1, [1, round((1-probWithout)*totalSamples)]);
    

    % Check to see how often samples are correctly categorized
    error = 1 - ((length(targetSamples(targetSamples > decisionRegion)) + length(nonTargetSamples(nonTargetSamples < decisionRegion))) / totalSamples);
end

% Find ROC arrays for Part 1A conditions
function [falsePositiveRate, truePositiveRate, specificSampleCoord] = ROCcurve(mu0, var0, mu1, var1, probWithout, decisionRegion)
    % Setup
    totalSamples = 10000;
    spacing = 1000;
    x = linspace(-10, 10, spacing);
    falsePositiveRate = zeros([1, 100]);
    truePositiveRate = zeros([1, 100]);
    specificSampleCoord = zeros([length(decisionRegion), 2]);
    
    % Generate proper amount of samples from each distribution
    nonTargetSamples = normrnd(mu0, var0, [1, round(probWithout*totalSamples)]);
    targetSamples = normrnd(mu1, var1, [1, round((1-probWithout)*totalSamples)]);
    
    %Run through all possible decision region choices to get ROC curve
    for i = 1:spacing
            falsePositiveRate(i) = length(nonTargetSamples(nonTargetSamples > x(i)))/ (round(probWithout*totalSamples));
            truePositiveRate(i) = length(targetSamples(targetSamples > x(i))) / (round((1-probWithout)*totalSamples));
    end
    
    %Use specific coordinate for the decision regions (mostly for part c)
    for i = 1:length(decisionRegion)
        specificSampleCoord(i, 1) = length(nonTargetSamples(nonTargetSamples > decisionRegion(i)))/ (round(probWithout*totalSamples));
        specificSampleCoord(i, 2) = length(targetSamples(targetSamples > decisionRegion(i))) / (round((1-probWithout)*totalSamples));
    end
end

% Find border of decision regions for Part 1E
function decisionRegion = findDecisionRegionE(mu0, var0, var1, probWithout)
    % Setup
    spacing = 10000;
    x = linspace(-10, 10, spacing);
    probWith = 1 - probWithout;
    
    % Get pdf when target is present and when it is not present
    yWithoutDetection = normpdf(x, mu0, var0)*probWithout;
    yWithDetection = normpdf(x, mu0, var1)*probWith;
    
    % Find where the threshold value is by seeing where the functions intersect
    decisionRegion = zeros([1,2]);
    for i = 1:spacing
        if (yWithoutDetection(i) <= yWithDetection(i))
            decisionRegion(2) = x(i);
        end
        
        if (yWithoutDetection(spacing + 1 - i) <= yWithDetection(spacing + 1 - i))
            decisionRegion(1) = x(spacing + 1 - i);
        end
    end
end

%Find experimental probability of error for the decision region in Part 1E
function error = experimentalProbErrorE(mu0, var0, var1, probWithout, decisionRegion)
    % Setup
    totalSamples = 100000;
    
    % Generate proper amount of samples from each distribution
    nonTargetSamples = normrnd(mu0, var0, [1, round(probWithout*totalSamples)]);
    targetSamples = normrnd(mu0, var1, [1, round((1-probWithout)*totalSamples)]);
    

    % Check to see how often samples are correctly categorized
    error = (length(nonTargetSamples((nonTargetSamples < decisionRegion(2)) & (nonTargetSamples > decisionRegion(1)))) + length((targetSamples(targetSamples > decisionRegion(2)))) + length((targetSamples(targetSamples < decisionRegion(1))))) / totalSamples;

end

% Find ROC arrays for Part 1E conditions
function [falsePositiveRate, truePositiveRate] = ROCcurveE(mu0, var0, var1, probWithout)
    % Setup
    totalSamples = 10000;
    spacing = 1000;
    x = linspace(-10, 10, spacing);
    falsePositiveRate = zeros([1, 100]);
    truePositiveRate = zeros([1, 100]);
    
    % Generate proper amount of samples from each distribution
    nonTargetSamples = normrnd(mu0, var0, [1, round(probWithout*totalSamples)]);
    targetSamples = normrnd(mu0, var1, [1, round((1-probWithout)*totalSamples)]);
    
    %Run through all possible decision region choices to get ROC curve
    for i = 1:spacing
            falsePositiveRate(i) = length(nonTargetSamples(abs(nonTargetSamples) < abs(x(i))))/ (round(probWithout*totalSamples));
            truePositiveRate(i) = length(targetSamples(abs(targetSamples) < abs(x(i)))) / (round((1-probWithout)*totalSamples));
    end
    
end

##### SOURCE END #####
--></body></html>